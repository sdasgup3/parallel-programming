Environment Management Routines
================================
MPI_Init(int *argc, char ***argv);
MPI_finalize();
MPI_Comm_size(MPI_Comm comm, int * size);
MPI_Comm_rank(MPI_Comm comm, int * rank);


Point to Point Communication Routines
======================================

Blocking Message Passing Routines
------------------------------------

arg_list_1 = (void* buffer, int count, MPI_Datatype type, int dest_or_src, int tag, MPI_Comm comm)

  MPI_Send
  MPI_Ssend
  MPI_Bsend

argr_list_2 = arg_list_1 + (MPI_Status* status )  
  MPI_Recv
  MPI_SRecv
  MPI_BRecv
    
arg_list_3 = (void* buffer, int size_in_bytes)
  MPI_Attach  
  MPI_Detach  

arg_list_4 = arg_list_1 - (MPI_Comm comm) +  arg_list_1 - (MPI_Comm comm)  + (MPI_Comm comm) + (MPI_Status* status) 
                  (for sender)                   (for receiver)  
  MPI_Sendrecv

arg_list_5 =  (void* buffer, int count, MPI_Datatype type, int dest, int sendtag, int source , int recvtag, MPI_Comm comm, MPI_Status* status)
  MPI_Sendrecv_replace

arg_list_6 = (MPI_Request *request, MPI_Status *status)
  MPI_Wait

arg_list_7 = (int count, MPI_Request * request_array, MPI_Status * status_array) 
  MPI_Waitall

arg_list_8 = (int count, MPI_Request * request_array, int* out_index, MPI_Status * status)
  MPI_Waitany

arg_list_9 = (int count, MPI_Request * request_array, int* outcount,  int* out_index_array, MPI_Status * status_array)
  MPI_Waitsome


Non-Blocking Message Passing Routines
---------------------------------------
arg_list_1 = (void* buffer, int count, MPI_Datatype type, int dest_or_src, int tag, MPI_Comm comm, MPI_Request *request)
  MPI_Isend
  MPI_Issend
  MPI_Ibsend
  MPI_Irecv

arg_list_2 = (MPI_Request *request, int* flag , MPI_Status *status)
    MPI_Test

arg_list_3 = (int count, MPI_Request *request_array, int* flag , MPI_Status *status_array)
    MPI_Testall

arg_list_4 = (int count, MPI_Request * request_array, int* out_index, int* flag,  MPI_Status * status)
    MPI_Testany
      
arg_list_5 = (int count, MPI_Request * request_array, int* outcount,  int* out_index_array, MPI_Status * status_array)
    MPI_Testsome

Collective Communication Routines
=================================
MPI_Barrier(MPI_Comm comm)

MPI_Bcast  (void* buffer, int count, MPI_Datatype type, int root , MPI_Comm comm );  


arg_list_1 = (void* sendbuffer, void* recvbuffer,  int count, MPI_Datatype type, MPI_Op op, int root , MPI_Comm comm)
  MPI_Reduce
  MPI_Scan 

arg_list_2 = arg_list_1 - (int root)
  MPI_Allreduce

arg_list_3 = (void *sendbuf, void *recvbuf, int *recvcnts, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm )
  MPI_Reduce_scatter
    recvcounts - integer array specifying the number of elements in result distributed to each process.  
    Array must be identical on all  call- ing processes.  

    MPI_REDUCE_SCATTER first does an element-wise reduction on vector of
    count  = sigma(i) {recvcnts(i)} elements in the send buffer defined by sendbuf, count and datatype. Next,
    the resulting vector of results is split into n disjoint segments, where n
    is the number of members in the group. Segment i contains recvcounts[i]
    elements. The ith segment is sent to process i and stored in the receive
    buffer defined by recvbuf, recvcounts[i] and datatype.

    The MPI_REDUCE_SCATTER routine is functionally equivalent to: A MPI_REDUCE
    operation function with count equal to the sum of recvcounts[i] followed by
    MPI_SCATTERV with sendcounts equal to recvcounts. However, a direct
    implementation may run faster. 

    Let recvcnts[3] = {2,2,1}
    So there will be 3 prcesses each having 2+2+1= 5 elements's vector
    P0  P1  P2
    x1  x2  x2
    y1  y2  y2
    z1  z2  z2
    p1  p2  p2
    q1  q2  q2

    After reduce
    P0
    sigma x
    sigma y
    sigma z
    sigma p
    sigma q

    After scatter
    P0          P1      P2
    sigma x   sigma z  sigma q 
    sigma y   sigma p

arg_list_4 =  (void *sendbuf, int send_count, MPI_Datatype send_type, void *recvbuf, int recv_count, MPI_Datatype recv_type, int root, MPI_Comm comm )    
  MPI_Gather
  MPI_Scatter

arg_list_5 = arg_list_4 - (int root)
  MPI_Allgather
  MPI_Alltoall

  MPI_Gatherv
  MPI_Scatterv
  MPI_Alltoallv



